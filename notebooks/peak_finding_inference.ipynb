{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636988f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Define CNN model \n",
    "# ---------------------------\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, filter1=128, filter2=32, dropout1=0.5, dropout2=0.3, dropout_fc=0.1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, filter1, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(filter1, filter2, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(filter2 * 61, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.dropout_fc = nn.Dropout(dropout_fc)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(filter1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(filter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(torch.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd409aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(1, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=1952, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (dropout_fc): Dropout(p=0.1, inplace=False)\n",
       "  (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Load trained peak-finding model\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peak_finding_model = CNN().to(device)\n",
    "peak_finding_model.load_state_dict(torch.load(\"../models/waveI_CNN.pth\", map_location=device))\n",
    "peak_finding_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. Normalize waveform before inference\n",
    "# ---------------------------\n",
    "def interpolate_and_smooth(wave, target_points=244, total_ms=10.0):\n",
    "    \"\"\"\n",
    "    Interpolates waveform to 244 points over 10 ms.\n",
    "    \"\"\"\n",
    "    orig_points = len(wave)\n",
    "    x_old = np.linspace(0, total_ms, orig_points)\n",
    "    x_new = np.linspace(0, total_ms, target_points)\n",
    "\n",
    "    if orig_points < target_points:\n",
    "        # Upsample with cubic spline\n",
    "        cs = CubicSpline(x_old, wave)\n",
    "        interp_wave = cs(x_new)\n",
    "    else:\n",
    "        # Downsample with linear interpolation\n",
    "        interp_wave = np.interp(x_new, x_old, wave)\n",
    "\n",
    "    return interp_wave\n",
    "\n",
    "def normalize_waveform(wave):\n",
    "    \"\"\"Standardize then scale to 0–1.\"\"\"\n",
    "    scaler1 = StandardScaler()\n",
    "    zscored = scaler1.fit_transform(wave.reshape(-1, 1)).flatten()\n",
    "    scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "    normalized = scaler2.fit_transform(zscored.reshape(-1, 1)).flatten()\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Peak finding with normalization + smoothing\n",
    "# ---------------------------\n",
    "def peak_finding(wave):\n",
    "    # Normalize before feeding to CNN\n",
    "    norm_wave = normalize_waveform(wave)\n",
    "    waveform_torch = torch.tensor(norm_wave, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    # CNN prediction for wave 1 index\n",
    "    outputs = peak_finding_model(waveform_torch)\n",
    "    prediction = int(round(outputs.detach().cpu().numpy()[0][0], 0))\n",
    "\n",
    "    # Gaussian smoothing on original (µV) waveform\n",
    "    smoothed_waveform = gaussian_filter1d(wave, sigma=1.0)\n",
    "\n",
    "    # Find peaks & troughs around prediction\n",
    "    n = 16\n",
    "    t = 7\n",
    "    window = 10\n",
    "    start_point = max(0, prediction - window)\n",
    "    smoothed_peaks, _ = find_peaks(smoothed_waveform[start_point:], distance=n)\n",
    "    smoothed_troughs, _ = find_peaks(-smoothed_waveform, distance=t)\n",
    "\n",
    "    peaks_within_ms = np.array([])\n",
    "    ms_cutoff = 0.25\n",
    "    while len(peaks_within_ms) == 0:\n",
    "        ms_window = int(ms_cutoff * len(smoothed_waveform) / 10)  \n",
    "        candidate_peaks = smoothed_peaks + start_point\n",
    "        within_ms_mask = np.abs(candidate_peaks - prediction) <= ms_window\n",
    "        peaks_within_ms = candidate_peaks[within_ms_mask]\n",
    "        ms_cutoff += 0.25\n",
    "    tallest_peak_idx = np.argmax(smoothed_waveform[peaks_within_ms])\n",
    "    pk1 = peaks_within_ms[tallest_peak_idx]\n",
    "\n",
    "    peaks = smoothed_peaks + start_point\n",
    "    peaks = peaks[peaks>pk1]\n",
    "    sorted_indices = np.argsort(smoothed_waveform[peaks])\n",
    "\n",
    "    highest_smoothed_peaks = np.sort(np.concatenate(\n",
    "        ([pk1], peaks[sorted_indices[-min(4, peaks.size):]])\n",
    "        )) \n",
    "    \n",
    "    relevant_troughs = []\n",
    "    for p in range(len(highest_smoothed_peaks)):\n",
    "        for tr in smoothed_troughs:\n",
    "            if tr > highest_smoothed_peaks[p]:\n",
    "                if p != 4:\n",
    "                    try:\n",
    "                        if tr < highest_smoothed_peaks[p + 1]:\n",
    "                            relevant_troughs.append(int(tr))\n",
    "                            break\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                else:\n",
    "                    relevant_troughs.append(int(tr))\n",
    "                    break\n",
    "\n",
    "    return highest_smoothed_peaks, np.array(relevant_troughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Convert peaks → latency & amplitude (Wave I only)\n",
    "# ---------------------------\n",
    "def extract_latency_amplitude(wave):\n",
    "    peaks, troughs, interp_wave = peak_finding(wave)\n",
    "\n",
    "    if len(peaks) == 0 or len(troughs) == 0:\n",
    "        return {\n",
    "            \"peak_idx\": None,\n",
    "            \"trough_idx\": None,\n",
    "            \"latency_ms\": None,\n",
    "            \"amplitude_uV\": None\n",
    "        }\n",
    "\n",
    "    # Take the FIRST peak + its first corresponding trough\n",
    "    p = int(peaks[0])\n",
    "    t = int(troughs[0]) if len(troughs) > 0 else None\n",
    "\n",
    "    latency_ms = p * (10.0 / 244)   # 10 ms / 244 samples\n",
    "    amplitude_uV = interp_wave[p] - interp_wave[t] if t is not None else None\n",
    "\n",
    "    return {\n",
    "        \"peak_idx\": p,\n",
    "        \"trough_idx\": t,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"amplitude_uV\": amplitude_uV\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6. Batch process folder\n",
    "# ---------------------------\n",
    "def batch_process(input_dir, output_csv=\"peak_results.csv\"):\n",
    "    all_results = []\n",
    "    for fname in os.listdir(input_dir):\n",
    "        if not fname.endswith(\".csv\"):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(input_dir, fname))\n",
    "        wave = df[\"Voltage\"].values[:244]\n",
    "        res = extract_latency_amplitude(wave)\n",
    "        for r in res:\n",
    "            r[\"file\"] = fname\n",
    "            all_results.append(r)\n",
    "    pd.DataFrame(all_results).to_csv(output_csv, index=False)\n",
    "    print(f\"Saved results to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 7. Optional visualization\n",
    "# ---------------------------\n",
    "def plot_example(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    wave = df[\"Voltage\"].values[:244]\n",
    "    res = extract_latency_amplitude(wave)\n",
    "    plt.plot(df[\"Time\"][:244], wave, label=\"ABR waveform\")\n",
    "    for r in res:\n",
    "        plt.scatter(r[\"latency_ms\"], wave[r[\"peak_idx\"]], c=\"r\", label=\"Peak\")\n",
    "        plt.scatter(r[\"latency_ms\"], wave[r[\"trough_idx\"]], c=\"b\", label=\"Trough\")\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"Voltage (µV)\")\n",
    "    plt.title(f\"{fname} | Peaks & Troughs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
