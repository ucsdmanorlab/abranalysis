{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABRWavesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        # Read the CSV file into a dataframe\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the waveform and mask for the given index\n",
    "        waveform = np.fromstring(self.df.iloc[idx]['waveform'][1:-1], dtype=float, sep=' ')\n",
    "        mask = np.fromstring(self.df.iloc[idx]['mask'][1:-1], dtype=float, sep=' ')\n",
    "        weight = self.df.iloc[idx]['sample_weight']\n",
    "\n",
    "        # Convert the waveform and mask to PyTorch tensors\n",
    "        waveform = torch.tensor(waveform, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        # We assume the target is the index of the peak (the maximum value in the mask)\n",
    "        peak_index = np.argmax(mask)\n",
    "        target = torch.tensor(peak_index, dtype=torch.float32)\n",
    "\n",
    "        return waveform, target, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN Model with tunable hyperparameters\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, filter1, filter2, dropout1, dropout2, dropout_fc):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=filter1, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=filter1, out_channels=filter2, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(filter2 * 61, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.dropout_fc = nn.Dropout(dropout_fc)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(filter1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(filter2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(nn.functional.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the combined CSV file\n",
    "combined_csv_file = 'wave_mask_pairs.csv'\n",
    "combined_df = pd.read_csv(combined_csv_file)\n",
    "\n",
    "# Determine sample weights based on the mask (lab source)\n",
    "# Use the filename length to distinguish between Marcotti Lab (<=3) and Manor Lab (>3)\n",
    "combined_df['lab'] = combined_df['filename'].apply(lambda x: 'Marcotti Lab' if len(x) <= 3 else 'Manor Lab')\n",
    "combined_df['mouse'] = combined_df['filename'].apply(lambda x: x.split()[0])\n",
    "\n",
    "# Get the unique mice and split into train and test sets\n",
    "unique_mice = combined_df['mouse'].unique()\n",
    "train_mice, test_mice = train_test_split(unique_mice, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter the dataset based on the mice splits\n",
    "train_df = combined_df[combined_df['mouse'].isin(train_mice)]\n",
    "test_df = combined_df[combined_df['mouse'].isin(test_mice)]\n",
    "\n",
    "# Further split the training data into train and validation sets (80-20 split)\n",
    "train_mice, val_mice = train_test_split(train_mice, test_size=0.2, random_state=42)\n",
    "train_df = combined_df[combined_df['mouse'].isin(train_mice)]\n",
    "val_df = combined_df[combined_df['mouse'].isin(val_mice)]\n",
    "\n",
    "lab_counts = train_df['lab'].value_counts()\n",
    "inverse_weights = {lab: 1.0 / count for lab, count in lab_counts.items()}\n",
    "weight_sum = sum(inverse_weights[lab] for lab in train_df['lab'])\n",
    "normalized_weights = {lab: (w / weight_sum) * len(train_df) for lab, w in inverse_weights.items()}\n",
    "train_df['sample_weight'] = train_df['lab'].map(normalized_weights)\n",
    "\n",
    "lab_counts = val_df['lab'].value_counts()\n",
    "inverse_weights = {lab: 1.0 / count for lab, count in lab_counts.items()}\n",
    "weight_sum = sum(inverse_weights[lab] for lab in val_df['lab'])\n",
    "normalized_weights = {lab: (w / weight_sum) * len(val_df) for lab, w in inverse_weights.items()}\n",
    "val_df['sample_weight'] = val_df['lab'].map(normalized_weights)\n",
    "\n",
    "lab_counts = test_df['lab'].value_counts()\n",
    "inverse_weights = {lab: 1.0 / count for lab, count in lab_counts.items()}\n",
    "weight_sum = sum(inverse_weights[lab] for lab in test_df['lab'])\n",
    "normalized_weights = {lab: (w / weight_sum) * len(test_df) for lab, w in inverse_weights.items()}\n",
    "test_df['sample_weight'] = test_df['lab'].map(normalized_weights)\n",
    "\n",
    "# Save the train, validation, and test sets to separate CSV files\n",
    "train_df.to_csv('train_wave_mask_pairs.csv', index=False)\n",
    "val_df.to_csv('val_wave_mask_pairs.csv', index=False)\n",
    "test_df.to_csv('test_wave_mask_pairs.csv', index=False)\n",
    "\n",
    "# Display the sizes of the train, validation, and test sets\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Total size: {len(train_df) + len(val_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load datasets for training, validation, and test\n",
    "train_dataset = ABRWavesDataset('train_wave_mask_pairs.csv')\n",
    "val_dataset = ABRWavesDataset('val_wave_mask_pairs.csv')\n",
    "test_dataset = ABRWavesDataset('test_wave_mask_pairs.csv')\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "# Hyperparameter search space\n",
    "filter1 = 128\n",
    "filter2 = 32\n",
    "dropout1 = 0.5\n",
    "dropout2 = 0.3\n",
    "dropout_fc = 0.1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "optimizer_name = \"Adam\"\n",
    "patience = 25\n",
    "\n",
    "# Model initialization\n",
    "model = CNN(filter1, filter2, dropout1, dropout2, dropout_fc).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) if optimizer_name == \"Adam\" else optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(1000):  # Early stopping will terminate if needed\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct_5, total_correct_10, total_samples = 0, 0, 0\n",
    "    \n",
    "    for waveform, target, weight in train_dataloader:\n",
    "        waveform, target, weight = waveform.to(device), target.to(device), weight.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(waveform).squeeze()\n",
    "        loss = criterion(outputs, target)\n",
    "        weighted_loss = (loss * weight).mean()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += weighted_loss.item()\n",
    "        \n",
    "        # Calculate accuracy within 5 and 10\n",
    "        with torch.no_grad():\n",
    "            predicted_peak = outputs.detach().cpu().numpy()\n",
    "            target_peak = target.cpu().numpy()\n",
    "            total_correct_5 += np.sum(np.abs(predicted_peak - target_peak) <= 5)\n",
    "            total_correct_10 += np.sum(np.abs(predicted_peak - target_peak) <= 10)\n",
    "            total_samples += waveform.size(0)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    accuracy_5 = (total_correct_5 / total_samples) * 100\n",
    "    accuracy_10 = (total_correct_10 / total_samples) * 100\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss, val_correct_5, val_correct_10, val_samples = 0.0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for waveform, target, weight in val_dataloader:\n",
    "            waveform, target, weight = waveform.to(device), target.to(device), weight.to(device)\n",
    "            outputs = model(waveform).squeeze()\n",
    "            loss = criterion(outputs, target)\n",
    "            weighted_loss = (loss * weight).mean()\n",
    "            val_loss += weighted_loss.item()\n",
    "            \n",
    "            predicted_peak = outputs.detach().cpu().numpy()\n",
    "            target_peak = target.cpu().numpy()\n",
    "            val_correct_5 += np.sum(np.abs(predicted_peak - target_peak) <= 5)\n",
    "            val_correct_10 += np.sum(np.abs(predicted_peak - target_peak) <= 10)\n",
    "            val_samples += waveform.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy_5 = (val_correct_5 / val_samples) * 100\n",
    "    val_accuracy_10 = (val_correct_10 / val_samples) * 100\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}: Accuracy within 5 points: {val_accuracy_5:2f}%, Accuracy within 10 points: {val_accuracy_10:2f}%')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "model.load_state_dict(best_model_state)  # Restore best weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Initialize lists to store predictions and ground truth values\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for waveform, target, _ in test_dataloader:  # Assuming you have a dataloader set up for evaluation\n",
    "        # Forward pass\n",
    "        outputs = model(waveform)\n",
    "        \n",
    "        # Convert predictions and ground truth tensors to numpy arrays\n",
    "        predictions.extend(outputs.squeeze().detach().numpy() * (10/244))\n",
    "        ground_truths.extend(target.numpy() * (10/244))\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(ground_truths, predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae:.10f}')\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(ground_truths, predictions, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.10f}')\n",
    "accuracy_within_10 = sum(abs(np.array(predictions) - np.array(ground_truths)) <= (100/244)) / len(predictions)\n",
    "print(f'Accuracy within {round(100/244,2)} ms or 10 points: {accuracy_within_10*100}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
